#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Structuralism
\end_layout

\begin_layout Author
Linas Vepstas
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset


\end_layout

\begin_layout Date
20 June 2025
\end_layout

\begin_layout Address
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset

BrainyBlaze
\end_layout

\begin_layout Abstract
A summary review of a structuralist, agent approach to machine consciousness.
\end_layout

\begin_layout Section*
Preface / Apologia
\end_layout

\begin_layout Standard
I will attempt to write as simply as possible.
 This means to some of the below might seem simplistic, obvious and well–known.
 If so, then its a mis–perception: attempting to turn the ideas below into
 working software is a half–finished project, with many difficult technical
 issues.
 These are accompanied by even harder questions of system architecture,
 design choice, and, at the most base level, the structural representation
 of what superficially seem like simple, easy obvious ideas.
\end_layout

\begin_layout Standard
Anyone familiar with philosophy or sociology will immediately recognize
 a broad range of familiar concepts: the signifier and signifcand, for example.
 Anyone familiar with mathematics will recognize a different set of familiar
 topics, such as monoidal categories and fragments of linear logic (the
 internal logic of dagger–symmetric tensor categories).
 These resemblances are not accidental; this text is being written in such
 a way as to evoke those ideas, and you the reader should think of them.
 There is much to be said and the author cannot say it all, so it is up
 to the reader to elaborate.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
It appears that the world is made of things and the relationship between
 things.
 These can be represented as graphs, with objects as vertices, and relations
 as edges.
 Graphs are decomposable into parts; a proper decomposition preserves the
 relationships by marking edges, so that the disassembled parts can be reassembl
ed appropriately.
 The marked cut edges imply that the graph parts resemble jigsaw puzzle
 pieces:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename reco-image/sparse-cut.eps
	width 80col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The task of intelligence, of an intelligent agent, is to discover these
 relationships.
 The agent can do so by means of perception; at the lowest levels these
 are concrete: eyes, ears, etc.
 but can also be abstract.
 For example, knowing some facts about geography and politics may still
 leave one ignorant of the relationship between the two, and may require
 a significant intellectual effort to understand this relationship.
 This is again an act of perception: the pattern of relationships between
 politics and geography has to be perceived, although the organ of perception
 is no longer as simple as 
\begin_inset Quotes eld
\end_inset

eyes and ears
\begin_inset Quotes erd
\end_inset

, but is also abstract.
\end_layout

\begin_layout Standard
An intelligent agent perceives and models the external universe, and constructs
 a provisional model of 
\begin_inset Quotes eld
\end_inset

what is outside
\begin_inset Quotes erd
\end_inset

.
 This model conventionally called 
\begin_inset Quotes eld
\end_inset

the world model
\begin_inset Quotes erd
\end_inset

, is necessarily incomplete, hypothetical, and riddled with errors.
 There may be more than 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 model that is active at a time, as one needs to entertain possibilities:
 
\begin_inset Quotes eld
\end_inset

it could be this or it could be that
\begin_inset Quotes erd
\end_inset

.
 This raises a minor philosophical question: is this 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 world–model, or 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

? After all, hypothetical relationships can be represented in a variety
 of formal logics; the 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

 possibilities can be collapsed down to one grand–total.
\end_layout

\begin_layout Subsection
Vector representations
\end_layout

\begin_layout Standard
The multiple possibilities have a natural representation as a vector; such
 a vector representation make natural contact with the conventional frameworks
 of artificial neural nets.
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename struct-image/jigsaw-tensor.eps
	width 90col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The above is meant to be a diagrammatic representation of a a vctor with
 three basis elements (eyes, lens, telescope) with three (real number) weights
 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

.
 The above is drawn diagrammatically; other common notations used in the
 literature are
\begin_inset Formula 
\begin{align*}
v & =w_{1}\left|\psi_{1}\right\rangle \oplus w_{2}\left|\psi_{2}\right\rangle \oplus w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\left|\psi_{1}\right\rangle +w_{2}\left|\psi_{2}\right\rangle +w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\hat{e}_{1}+w_{2}\hat{e}_{2}+w_{3}\hat{e}_{3}\\
 & =w_{1}e_{1}+w_{2}e_{2}+w_{3}e_{3}\\
 & =\left[w_{1},w_{2},w_{3}\right]\\
 & =X_{1}p\left(X\vert x=X_{1}\right)+X_{2}p\left(X\vert x=X_{2}\right)+X_{3}p\left(X\vert x=X_{3}\right)
\end{align*}

\end_inset

All of these mean exactly the same thing; the notation varies according
 to the style and preference of the authors.
 The last line is meant to be typical of notation used in Bayesian texts,
 where one talks of conditional probabilities; the first line is typical
 of texts on quantum (in which case the weights are complex numbers) and
 sometimes seen in tensor categories and category–theoretic texts.
 The 
\begin_inset Formula $\hat{e}_{k}$
\end_inset

 are simply unit basis vectors, in the conventional linear algebra sense.
\end_layout

\begin_layout Standard
For the present purposes, these differences are immaterial; yes, of course,
 quantum is not the same as Bayesianism, but for now this does not matter.
 The distinctions will be drawn later; for now, the emphasis is that graphical
 elements, such as jigsaws, can be formally combined to form vectors, and
 that distinct graphs are the 
\begin_inset Quotes eld
\end_inset

basis vecttors
\begin_inset Quotes erd
\end_inset

 of the vector space.
\end_layout

\begin_layout Standard
Computationally, the graphical elements depicted in the diagram should be
 stored somewhere, for example, in a graph database; and alongside each
 there should be a floating–point value.
 It can be stored with the graph, or outside the graph, as a distinct vector;
 this is a software engineering issue and not a fundamental issue.
\end_layout

\begin_layout Standard
What do the vector weights 
\begin_inset Quotes eld
\end_inset

mean
\begin_inset Quotes erd
\end_inset

? These are perhaps context–dependent.
 If one is visiting an astronomical observatory, it might make sense to
 set 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.05,0.05,0.9\right)$
\end_inset

 but if one is taking a walk in the park, then perhaps 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.9,0.1,0\right)$
\end_inset

 with 
\begin_inset Formula $w_{2}$
\end_inset

 non–zero because one wears eye–glasses.
 But the 
\begin_inset Quotes eld
\end_inset

actual meaning
\begin_inset Quotes erd
\end_inset

 of this vector is somewhat immaterial; the weights will ultimately be determine
d by whatever algorithm one uses, and whatever 
\begin_inset Quotes eld
\end_inset

meaning
\begin_inset Quotes erd
\end_inset

 that algorithm assigns to weights.
 The algorithm might be some deep–learning neural net transformer; it might
 be some Bayesian system; it might be a frequentist counting algorithm.
\end_layout

\begin_layout Standard
Later on, we will make the conceptual jump that one may work with many different
 algorithms 
\begin_inset Quotes eld
\end_inset

at the same time
\begin_inset Quotes erd
\end_inset

, and that one can (meaningfully) arrange these into a vector.
 The weights 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 might indicate how many CPU cycles should be devoted to algorithms 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 but they might also mean how often these algorithms 
\begin_inset Quotes eld
\end_inset

get the right answer
\begin_inset Quotes erd
\end_inset

.
 The perceptual framing at this more abstract level is 
\begin_inset Quotes eld
\end_inset

how can I perceive which algorithm is the best one?
\begin_inset Quotes erd
\end_inset

 The intelligent agent might be an artificial scientist, running experiments
 to determine which of the scientific hypothesis 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 are correct.
\end_layout

\begin_layout Standard
The implied software design principle here is that the software system must
 be capable of representing 
\begin_inset Quotes eld
\end_inset

objects
\begin_inset Quotes erd
\end_inset

 like 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 in some abstract way, but also working with vectors and matrices and tensors
 in a natural way: this is the 
\begin_inset Quotes eld
\end_inset

neuro–symbolic
\begin_inset Quotes erd
\end_inset

 idea: the 
\begin_inset Formula $e_{1},e_{2},e_{3}$
\end_inset

 are symbolic, the 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

 are neuro.
\end_layout

\begin_layout Standard
The implied context of this text is that the appropriate software system
 for this work is the AtomSpace.
 This claim is founded on the author having spent more than a decade to
 realize the above (and more, some of which touched on below) in a practical,
 efficient, usable and useful way.
 The design is imperfect; new insights are gained regularly.
 Of course, other software frameworks are possible; the engineering task
 is to avoid common pitfalls, design oversights and, of course, re–inventing
 the wheel.
 The AtomSpace is not the be–all, end–all, and it might not be at all suitable
 for whatever you have in mind.
 In such a case, use what you need to use.
 For myself, speaking personally, it is the best that I've got, at the moment.
 The actual stress of daily use does mean that the process is evolutionary,
 and that revisions and enhancements are ongoing.
 Perhaps at a lower rate than before, perhaps limited to narrower sub–modules
 and subsystems.
 The theory drives the practice drives the code and the driving also happens
 in the reverse.
\end_layout

\begin_layout Subsection
Frequentist vector representations
\end_layout

\begin_layout Standard
In the narrow context of deep–learning neural nets, it is widely understood
 the vector embeddings work extremely well, taking the form of modern LLM's
 and a huge variety of applications of transformers to a broad range of
 topics.
 What is not well known is that there are other ways of getting at least
 some of these results.
\end_layout

\begin_layout Standard
The goal of this section is to show how to obtain these results, and to
 provide a very short review of linguistics.
 Now, of course, present–day LLM's far exceed the abilities of present–day
 traditional linguistics; and the latter have fallen into disfavor threatening
 obscurity.
 The ideas developed immediately below is that there is another path, this
 path is explicitly symbolic, and that this path, to the extent that it
 has been pursued, actually reproduces at least some neural–net results.
 
\end_layout

\begin_layout Standard
The original prototype system was WORD2VEC and the prototypical example
 is the vector relation King–Man+Woman=Queen.
 Here, the word–tokens for the words 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Queen
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Woman
\begin_inset Quotes erd
\end_inset

 were given vector embeddings, and the system, a multi–layer recurrent neural
 net (RNN) was trained on a large body of text, using gradient–descent methods.
 After training, the resulting vectors have several interesting properties:
 they exhibit 
\begin_inset Quotes eld
\end_inset

associative memory
\begin_inset Quotes erd
\end_inset

, in that nearby vectors appear to be related, and, much more surprisingly,
 vector subtraction, i.e.
 subtracting the vector for 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 from the vector for 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

 seems to give a vector representation for 
\begin_inset Quotes eld
\end_inset

ruler
\begin_inset Quotes erd
\end_inset

 that is gender–neutral, in that 
\begin_inset Quotes eld
\end_inset

Queen–Woman
\begin_inset Quotes erd
\end_inset

 is another vector that is close to 
\begin_inset Quotes eld
\end_inset

King–Man
\begin_inset Quotes erd
\end_inset

.
 Here, 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 means that the cosine product (the vector product) is approximately 1.0.
 Again, this is all well–known.
\end_layout

\begin_layout Standard
What is not well–known is that the above results can be obtained in at least
 one very different way: by frequentist counting.
 Below follows a short sketch of how this is done.
 The focus and lesson here is that the algorithm is not only very different
 from that of RNN's, but is also explicitly graphical (symbolic) in nature.
 This provides the leverage needed to make the jump from neuro to symbolic
 (structural) and back.
\end_layout

\begin_layout Standard
The algorithm described here was provided by Deniz Yuret in his PhD thesis
 in the late 1990's.
 A variety of related formulations were explored by a number of authors
 (including Dekang Lin and Pedro Domingos) over dozens of publications (if
 not more).
 None appear to have made the full conceptual leap being described here,
 although they all danced very close.
 I don't entirely understand why; this is perhaps the intermittent nature
 of scientific discovery.
\end_layout

\begin_layout Standard
The Yuret algorithm starts with a text training corpus, and simply 
\emph on
counts
\emph default
 nearby word–pairs.
 Nearby simply means 
\begin_inset Quotes eld
\end_inset

within a small context window
\begin_inset Quotes erd
\end_inset

, traditionally six words wide.
 This count is written as 
\begin_inset Formula $N\left(w_{a},w_{b}\right)$
\end_inset

 for words 
\begin_inset Formula $w_{a},w_{b}$
\end_inset

 (no relation to the weights 
\begin_inset Formula $w$
\end_inset

 in the previous section.) Yuret defines the mutual information (MI) associated
 with a word–pair as 
\begin_inset Formula 
\[
MI\left(w_{a},w_{b}\right)=\log_{2}\frac{N\left(w_{a},w_{b}\right)N\left(*,*\right)}{N\left(w_{a},*\right)N\left(*,w_{b}\right)}
\]

\end_inset

where 
\begin_inset Formula $*$
\end_inset

is 
\begin_inset Quotes eld
\end_inset

wild-card
\begin_inset Quotes erd
\end_inset

, so that 
\begin_inset Formula $N\left(w_{a},*\right)=\sum_{u}N\left(w_{a},u\right)$
\end_inset

 summed over all right–words 
\begin_inset Formula $u$
\end_inset

.
 Yuret observes that words that 
\begin_inset Quotes eld
\end_inset

go together
\begin_inset Quotes erd
\end_inset

 have a high MI.
 For example, he finds that the MI for 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 to be very high; this has to do with the presence of the word–pair 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 in many newspaper stories of the era.
 
\end_layout

\begin_layout Standard
This allows him to propose a parsing algorithm that gives results rather
 close to those proposed by linguists, in which a word such as 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

noun
\begin_inset Quotes erd
\end_inset

, while 
\begin_inset Quotes eld
\end_inset

Northern
\begin_inset Quotes erd
\end_inset

 is taken as an adjective, modifying 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

.
 The resulting parses are of the form of a dependency grammar, 
\emph on
a la
\emph default
 Lucien Tesnière, and more generally along the many rich elaborations of
 dependency grammars over many decades, hundreds of books and far more papers.
 An example parse, taken from Yuret's thesis, is shown below.
 The numbers above the word pairs are the numerical MI value obtained from
 counting.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename agi-2022-tex/Yuret.eps

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The parse is obtained by considering all possible trees joining the words,
 and selecting the tree for which the total MI is the largest possible.
 Basic familiarity with linguistics shows that this parse is exactly correct:
 all of the dependencies are indicated exactly as they should be.
 This is remarkable, because it shows that the toil and trouble that linguists
 have gone through to assemble lexis (dictionaries) by hand, indicating
 subject–verb–noun relations, adjective–noun relations, determiner–noun
 relations, prepositions, etc.
 can be fully automated simply by counting and noting the frequency.
 This is a remarkable achievement.
 This result is explored by many authors under the rubric of 
\begin_inset Quotes eld
\end_inset

maximal planar parsing
\begin_inset Quotes erd
\end_inset

 and is tied to more general principles of 
\begin_inset Quotes eld
\end_inset

maximum entropy principles
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
There are two things that Yuret does not do, and apparently no one else
 of that era, exploring variations on this theme, seem to explore or discover.
 One is that there is a vector embedding, and that this embedding reproduces
 exactly the famous WORD2VEC result of King–Man+Woman=Queen.
 What's odd is that, chronologically, this result could have been discovered
 earlier, but wasn't.
 The second important step is that more complex structures can be recursively
 discovered.
\end_layout

\begin_layout Standard
First, lets look at the vector embedding.
 It is nearly trivial, once you see it.
 The vector to be written is 
\begin_inset Formula 
\[
\left|King\right\rangle =\sum_{a}c_{a}\left|w_{a}\right\rangle 
\]

\end_inset

where the sum extends over all words in the vocabulary.
 Each word is associated with a basis vector, written here as 
\begin_inset Formula $\left|w_{a}\right\rangle $
\end_inset

.
 This is in the sense of the diagram up top, except that the graph is a
 trivial graph: it is just a single vertex, having the word as the vertex
 label.
 The 
\begin_inset Formula $c_{a}$
\end_inset

 are just floating–point numbers.
 To belabor the point, the sum over the vocabulary can be written as 
\begin_inset Formula 
\[
\left|King\right\rangle =c_{dog}\left|dog\right\rangle +c_{tall}\left|tall\right\rangle +c_{other}\left|other\right\rangle +\cdots
\]

\end_inset

and the Ansatz is that the constants 
\begin_inset Formula $c_{a}$
\end_inset

 can be taken to be 
\begin_inset Formula $c_{a}=MI\left(w_{a},King\right)$
\end_inset

.
 This gives a bona–fide vector, and, as a vector one can take dot products,
 and compute the cosine angles between them, and, when one takes care to
 normalize the vectors to unit length, one recovers the prototypical vector
 embedding result King–Man+Woman=Queen.
 This holds entirely without any appeal to RNN's or any of the other mechanics
 of deep learning.
 Nor is there any immediate need or appeals to free energy, Boltzmann distributi
ons, Markov properties, integrated information theory, Tononi's Phi or any
 other advanced conceptual frameworks.
 It can be noted (should be noted) that 
\begin_inset Formula $MI\left(w_{a},w_{b}\right)$
\end_inset

 is a matrix (a 2–tensor) and that is is kind–of 
\begin_inset Quotes eld
\end_inset

Markovian
\begin_inset Quotes erd
\end_inset

 in a certain sense, although just how is requires a much fuller theory.
\end_layout

\begin_layout Standard
The second 
\begin_inset Quotes eld
\end_inset

obvious
\begin_inset Quotes erd
\end_inset

 extension to the maximum entropy principle that no one seems to make is
 that it can be extended to obtain the full dependency lexis that is the
 centerpiece of traditional linguistics.
 The basic idea is best illustrated with a very simple diagram:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename skimage/disjunct.eps
	width 60col%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The top row of the diagram show a parse obtained via Yuret's frequentist
 maximal MI parsing algorithm.
 The lower row merely applies a pair of scissors to cut the graph into jigsaws,
 and labels the cut ends so that they can be reconstructed.
 This disgram should be compared to the figure below, taken from the 1991
 paper describing Link Grammar (LG):
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename reco-image/link-grammar.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
This figure explicitly shows the jigsaw connectors in a jigsaw–shape form.
 The point of Link Grammar is that if one abides by the rule that matching
 connectors must be mated according to the obvious matching rules, the generated
 sentences are always grammatical.
 The connectors are labelled with the letters D, S, O, standing for Determiner,
 Subject and Object.
 The intransitive verb 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 must take a subject (which is a noun); the transitive verb 
\begin_inset Quotes eld
\end_inset

chased
\begin_inset Quotes erd
\end_inset

 takes a subject and an object (both nouns).
\end_layout

\begin_layout Standard
As a theory of grammar, Link Grammar 
\begin_inset Quotes eld
\end_inset

works
\begin_inset Quotes erd
\end_inset

: it has fully developed and complete dictionaries for English, Russian
 and Thai.
 It has a demo dictionaries for German, Arabic and Persian and proof–of–concept
 dictionaries for Vietnamese, Turkish, Lithuanian and Indonesian.
 It works.
\end_layout

\begin_layout Standard
A few quick remarks about notation are in order.
 The formal Link Grammar parse for the prior example would be
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename skimage/parse.ps
	lyxscale 160
	width 30col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align left
The lexical entries, in conventional LG notation are
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball:  D- & O-;
\end_layout

\begin_layout Plain Layout

	Kevin: S+;
\end_layout

\begin_layout Plain Layout

	the:   D+;
\end_layout

\begin_layout Plain Layout

	threw: S- & O+;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
The capital letters, together with a 
\family typewriter
+
\family default
/
\family typewriter
-
\family default
 sign, are called 
\begin_inset Quotes eld
\end_inset

connectors
\begin_inset Quotes erd
\end_inset

; a pair of connectors form a 
\begin_inset Quotes eld
\end_inset

link
\begin_inset Quotes erd
\end_inset

.
 The 
\family typewriter
+
\family default
/
\family typewriter
-
\family default
 sign indicates a left–right directionality: for example, the 
\family typewriter
S+
\family default
 connector can only connect to the right; it must mate with an 
\family typewriter
S-
\family default
 connector to form an 
\family typewriter
S
\family default
 link.
 A valid parse exists if and only if all available connectors are paired
 up.
 The construction 
\family typewriter
S+ & O-
\family default
 is called a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

; the name has a historical basis that is of no particular concern here.
 During parsing, all connectors in a disjunct must be satisfied.
\end_layout

\begin_layout Standard
Each lexical entry is a word–disjunct pair; they are of the general form
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	word: A- & B+ & C+ & ...;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is the notation used in the original LG papers.
 It turns out that this is also an active topic of research in quantum computing
; Bob Coecke gives a quantum algorithm that can parse LG in linear time.
 The notation used in those papers would write 
\begin_inset Formula 
\[
\left|word\right\rangle =\left\langle A\right|\otimes\left|B\right\rangle \otimes\left|C\right\rangle \otimes\cdots
\]

\end_inset

Please note that although the notation is quite different, the intended
 interpretation is exactly the same.
 This is not an accident: it turns out that algebraically, the system of
 jigsaw connectors forms a semi–commutative monoid; the internal logic of
 this category embeds a fragment of linear logic; this fragment is the same
 as that for history and trace monoids used to describe communicating computing
 networks, and the mutex locks needed to serialize access to data.
 But linear logic is the internal logic of dagger–compact categories, with
 are the categories that describe Hilbert spaces and quantum mechanics more
 generally.
 It is for this reason that one can discover quantum computing algorithms
 that perform linguistic parsing: the logic of jigsaw assembly is not just
 a subset of quantum, but is also the correct language for describing communicat
ing computer processes and the message–passing between them.
 Informally, you can think of a jigsaw piece as a computer program, and
 the jigsaw connectors as the ports through which it can communicate.
\end_layout

\begin_layout Standard
This last paragraph consisted of a waterfall of jargon and buzzwords.
 The intent of this paragraph is to make the statement that the jigsaw paradigm
 is fundamental: it has algebraic properties that are exactly what is needed
 to describe a vast array of phenomena.
 The use of jigsaws for linguistics is noted as early as the 1960's; the
 utility for computing isn't noted till decades later, and the connection
 to quantum is not noted until the idea of string diagrams and the Mike
 Stay / John Baez 
\begin_inset Quotes eld
\end_inset

Rosetta Stone
\begin_inset Quotes erd
\end_inset

 is written.
 Perhaps this sounds somehow deep and magical; actually, its not.
 It is simply the observation that if you take a pair of scissors to a graph,
 and cut up the edges, but preserve the labels, you get jigsaws.
 Since, in a sense.
 
\begin_inset Quotes eld
\end_inset

the universe is just a collection of objects and the relations between them
\begin_inset Quotes erd
\end_inset

, it is not a surprise that 
\begin_inset Quotes eld
\end_inset

everything is describable with jigsaws
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
A short note about algorithms.
 The diagram above yielded a jigsaw
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball: the- & threw-;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
whereas the desired LG representation would be
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball: D- & O-;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
How can one get from individual words, to connector categories? Easy: clustering.
 On can again perform frequentist counting, obtain MI , compute cosine products,
 and create clusters from the resulting 
\begin_inset Quotes eld
\end_inset

associative memory
\begin_inset Quotes erd
\end_inset

.
 This has been done in the OpenCog Learn project, circa 2016–2020, and it
 works fine.
 The tooling for all this exists for the AtomSpace; however, it is now old,
 and there is good reason to redesign it from the ground up, as the next
 sections will review.
\end_layout

\begin_layout Standard
BTW, once again, one has embedding vectors: one can write (for example)
\begin_inset Formula 
\[
\left|King\right\rangle =c_{1}\left|the\right\rangle \otimes\left|threw\right\rangle +c_{2}\left|ruled\right\rangle \otimes\left|Poland\right\rangle +\cdots
\]

\end_inset

and after computing the constants 
\begin_inset Formula $c_{1},c_{2},\cdots$
\end_inset

 appropriately, one will find, once again, that King–Man+Woman=Queen holds
 as a vector relation.
\end_layout

\begin_layout Standard
c+3one can get
\end_layout

\begin_layout Standard
xxx
\end_layout

\begin_layout Standard
vectorGPU
\end_layout

\begin_layout Subsection
sensorimotor 
\end_layout

\begin_layout Standard
xxxmotor
\end_layout

\begin_layout Standard
attention 
\end_layout

\begin_layout Standard
associative memory
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
