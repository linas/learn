#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Structuralism
\end_layout

\begin_layout Author
Linas Vepstas
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset


\end_layout

\begin_layout Date
20 June 2025
\end_layout

\begin_layout Address
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset

BrainyBlaze
\end_layout

\begin_layout Abstract
A summary review of a structuralist, agent approach to machine consciousness.
\end_layout

\begin_layout Section*
Preface / Apologia
\end_layout

\begin_layout Standard
I will attempt to write as simply as possible.
 This means to some of the below might seem simplistic, obvious and well–known.
 If so, then its a mis–perception: attempting to turn the ideas below into
 working software is a half–finished project, with many difficult technical
 issues.
 These are accompanied by even harder questions of system architecture,
 design choice, and, at the most base level, the structural representation
 of what superficially seem like simple, easy obvious ideas.
\end_layout

\begin_layout Standard
Anyone familiar with philosophy or sociology will immediately recognize
 a broad range of familiar concepts: the signifier and signifcand, for example.
 Anyone familiar with mathematics will recognize a different set of familiar
 topics, such as monoidal categories and fragments of linear logic (the
 internal logic of dagger–symmetric tensor categories).
 These resemblances are not accidental; this text is being written in such
 a way as to evoke those ideas, and you the reader should think of them.
 There is much to be said and the author cannot say it all, so it is up
 to the reader to elaborate.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
It appears that the world is made of things and the relationship between
 things.
 These can be represented as graphs, with objects as vertices, and relations
 as edges.
 Graphs are decomposable into parts; a proper decomposition preserves the
 relationships by marking edges, so that the disassembled parts can be reassembl
ed appropriately.
 The marked cut edges imply that the graph parts resemble jigsaw puzzle
 pieces:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename reco-image/sparse-cut.eps
	width 80col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The task of intelligence, of an intelligent agent, is to discover these
 relationships.
 The agent can do so by means of perception; at the lowest levels these
 are concrete: eyes, ears, etc.
 but can also be abstract.
 For example, knowing some facts about geography and politics may still
 leave one ignorant of the relationship between the two, and may require
 a significant intellectual effort to understand this relationship.
 This is again an act of perception: the pattern of relationships between
 politics and geography has to be perceived, although the organ of perception
 is no longer as simple as 
\begin_inset Quotes eld
\end_inset

eyes and ears
\begin_inset Quotes erd
\end_inset

, but is also abstract.
\end_layout

\begin_layout Standard
An intelligent agent perceives and models the external universe, and constructs
 a provisional model of 
\begin_inset Quotes eld
\end_inset

what is outside
\begin_inset Quotes erd
\end_inset

.
 This model conventionally called 
\begin_inset Quotes eld
\end_inset

the world model
\begin_inset Quotes erd
\end_inset

, is necessarily incomplete, hypothetical, and riddled with errors.
 There may be more than 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 model that is active at a time, as one needs to entertain possibilities:
 
\begin_inset Quotes eld
\end_inset

it could be this or it could be that
\begin_inset Quotes erd
\end_inset

.
 This raises a minor philosophical question: is this 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 world–model, or 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

? After all, hypothetical relationships can be represented in a variety
 of formal logics; the 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

 possibilities can be collapsed down to one grand–total.
\end_layout

\begin_layout Subsection
Vector representations
\end_layout

\begin_layout Standard
The multiple possibilities have a natural representation as a vector; such
 a vector representation make natural contact with the conventional frameworks
 of artificial neural nets.
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename struct-image/jigsaw-tensor.eps
	width 90col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The above is meant to be a diagrammatic representation of a a vctor with
 three basis elements (eyes, lens, telescope) with three (real number) weights
 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

.
 The above is drawn diagrammatically; other common notations used in the
 literature are
\begin_inset Formula 
\begin{align*}
v & =w_{1}\left|\psi_{1}\right\rangle \oplus w_{2}\left|\psi_{2}\right\rangle \oplus w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\left|\psi_{1}\right\rangle +w_{2}\left|\psi_{2}\right\rangle +w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\hat{e}_{1}+w_{2}\hat{e}_{2}+w_{3}\hat{e}_{3}\\
 & =w_{1}e_{1}+w_{2}e_{2}+w_{3}e_{3}\\
 & =\left[w_{1},w_{2},w_{3}\right]\\
 & =X_{1}p\left(X\vert x=X_{1}\right)+X_{2}p\left(X\vert x=X_{2}\right)+X_{3}p\left(X\vert x=X_{3}\right)
\end{align*}

\end_inset

All of these mean exactly the same thing; the notation varies according
 to the style and preference of the authors.
 The last line is meant to be typical of notation used in Bayesian texts,
 where one talks of conditional probabilities; the first line is typical
 of texts on quantum (in which case the weights are complex numbers) and
 sometimes seen in tensor categories and category–theoretic texts.
 The 
\begin_inset Formula $\hat{e}_{k}$
\end_inset

 are simply unit basis vectors, in the conventional linear algebra sense.
\end_layout

\begin_layout Standard
For the present purposes, these differences are immaterial; yes, of course,
 quantum is not the same as Bayesianism, but for now this does not matter.
 The distinctions will be drawn later; for now, the emphasis is that graphical
 elements, such as jigsaws, can be formally combined to form vectors, and
 that distinct graphs are the 
\begin_inset Quotes eld
\end_inset

basis vecttors
\begin_inset Quotes erd
\end_inset

 of the vector space.
\end_layout

\begin_layout Standard
Computationally, the graphical elements depicted in the diagram should be
 stored somewhere, for example, in a graph database; and alongside each
 there should be a floating–point value.
 It can be stored with the graph, or outside the graph, as a distinct vector;
 this is a software engineering issue and not a fundamental issue.
\end_layout

\begin_layout Standard
What do the vector weights 
\begin_inset Quotes eld
\end_inset

mean
\begin_inset Quotes erd
\end_inset

? These are perhaps context–dependent.
 If one is visiting an astronomical observatory, it might make sense to
 set 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.05,0.05,0.9\right)$
\end_inset

 but if one is taking a walk in the park, then perhaps 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.9,0.1,0\right)$
\end_inset

 with 
\begin_inset Formula $w_{2}$
\end_inset

 non–zero because one wears eye–glasses.
 But the 
\begin_inset Quotes eld
\end_inset

actual meaning
\begin_inset Quotes erd
\end_inset

 of this vector is somewhat immaterial; the weights will ultimately be determine
d by whatever algorithm one uses, and whatever 
\begin_inset Quotes eld
\end_inset

meaning
\begin_inset Quotes erd
\end_inset

 that algorithm assigns to weights.
 The algorithm might be some deep–learning neural net transformer; it might
 be some Bayesian system; it might be a frequentist counting algorithm.
\end_layout

\begin_layout Standard
Later on, we will make the conceptual jump that one may work with many different
 algorithms 
\begin_inset Quotes eld
\end_inset

at the same time
\begin_inset Quotes erd
\end_inset

, and that one can (meaningfully) arrange these into a vector.
 The weights 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 might indicate how many CPU cycles should be devoted to algorithms 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 but they might also mean how often these algorithms 
\begin_inset Quotes eld
\end_inset

get the right answer
\begin_inset Quotes erd
\end_inset

.
 The perceptual framing at this more abstract level is 
\begin_inset Quotes eld
\end_inset

how can I perceive which algorithm is the best one?
\begin_inset Quotes erd
\end_inset

 The intelligent agent might be an artificial scientist, running experiments
 to determine which of the scientific hypothesis 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 are correct.
\end_layout

\begin_layout Standard
The implied software design principle here is that the software system must
 be capable of representing 
\begin_inset Quotes eld
\end_inset

objects
\begin_inset Quotes erd
\end_inset

 like 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 in some abstract way, but also working with vectors and matrices and tensors
 in a natural way: this is the 
\begin_inset Quotes eld
\end_inset

neuro–symbolic
\begin_inset Quotes erd
\end_inset

 idea: the 
\begin_inset Formula $e_{1},e_{2},e_{3}$
\end_inset

 are symbolic, the 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

 are neuro.
\end_layout

\begin_layout Standard
The implied context of this text is that the appropriate software system
 for this work is the AtomSpace.
 This claim is founded on the author having spent more than a decade to
 realize the above (and more, some of which touched on below) in a practical,
 efficient, usable and useful way.
 The design is imperfect; new insights are gained regularly.
 Of course, other software frameworks are possible; the engineering task
 is to avoid common pitfalls, design oversights and, of course, re–inventing
 the wheel.
 The AtomSpace is not the be–all, end–all, and it might not be at all suitable
 for whatever you have in mind.
 In such a case, use what you need to use.
 For myself, speaking personally, it is the best that I've got, at the moment.
 The actual stress of daily use does mean that the process is evolutionary,
 and that revisions and enhancements are ongoing.
 Perhaps at a lower rate than before, perhaps limited to narrower sub–modules
 and subsystems.
 The theory drives the practice drives the code and the driving also happens
 in the reverse.
\end_layout

\begin_layout Subsection
Frequentist vector representations
\end_layout

\begin_layout Standard
In the narrow context of deep–learning neural nets, it is widely understood
 the vector embeddings work extremely well, taking the form of modern LLM's
 and a huge variety of applications of transformers to a broad range of
 topics.
 What is not well known is that there are other ways of getting at least
 some of these results.
\end_layout

\begin_layout Standard
The original prototype system was WORD2VEC and the prototypical example
 is the vector relation King–Man+Woman=Queen.
 Here, the word–tokens for the words 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Queen
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Woman
\begin_inset Quotes erd
\end_inset

 were given vector embeddings, and the system, a multi–layer recurrent neural
 net (RNN) was trained on a large body of text, using gradient–descent methods.
 After training, the resulting vectors have several interesting properties:
 they exhibit 
\begin_inset Quotes eld
\end_inset

associative memory
\begin_inset Quotes erd
\end_inset

, in that nearby vectors appear to be related, and, much more surprisingly,
 vector subtraction, i.e.
 subtracting the vector for 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 from the vector for 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

 seems to give a vector representation for 
\begin_inset Quotes eld
\end_inset

ruler
\begin_inset Quotes erd
\end_inset

 that is gender–neutral, in that 
\begin_inset Quotes eld
\end_inset

Queen–Woman
\begin_inset Quotes erd
\end_inset

 is another vector that is close to 
\begin_inset Quotes eld
\end_inset

King–Man
\begin_inset Quotes erd
\end_inset

.
 Here, 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 means that the cosine product (the vector product) is approximately 1.0.
 Again, this is all well–known.
\end_layout

\begin_layout Standard
What is not well–known is that the above results can be obtained in at least
 one very different way: by frequentist counting.
 Below follows a short sketch of how this is done.
 The focus and lesson here is that the algorithm is not only very different
 from that of RNN's, but is also explicitly graphical (symbolic) in nature.
 This provides the leverage needed to make the jump from neuro to symbolic
 (structural) and back.
\end_layout

\begin_layout Standard
The algorithm described here was provided by Deniz Yuret in his PhD thesis
 in the late 1990's.
 A variety of related formulations were explored by a number of authors
 (including Dekang Lin and Pedro Domingos) over dozens of publications (if
 not more).
 None appear to have made the full conceptual leap being described here,
 although they all danced very close.
 I don't entirely understand why; this is perhaps the intermittent nature
 of scientific discovery.
\end_layout

\begin_layout Standard
The Yuret algorithm starts with a text training corpus, and simply 
\emph on
counts
\emph default
 nearby word–pairs.
 Nearby simply means 
\begin_inset Quotes eld
\end_inset

within a small context window
\begin_inset Quotes erd
\end_inset

, traditionally six words wide.
 This count is written as 
\begin_inset Formula $N\left(w_{a},w_{b}\right)$
\end_inset

 for words 
\begin_inset Formula $w_{a},w_{b}$
\end_inset

 (no relation to the weights 
\begin_inset Formula $w$
\end_inset

 in the previous section.) Yuret defines the mutual information (MI) associated
 with a word–pair as 
\begin_inset Formula 
\[
MI\left(w_{a},w_{b}\right)=\log_{2}\frac{N\left(w_{a},w_{b}\right)N\left(*,*\right)}{N\left(w_{a},*\right)N\left(*,w_{b}\right)}
\]

\end_inset

where 
\begin_inset Formula $*$
\end_inset

is 
\begin_inset Quotes eld
\end_inset

wild-card
\begin_inset Quotes erd
\end_inset

, so that 
\begin_inset Formula $N\left(w_{a},*\right)=\sum_{u}N\left(w_{a},u\right)$
\end_inset

 summed over all right–words 
\begin_inset Formula $u$
\end_inset

.
 Yuret observes that words that 
\begin_inset Quotes eld
\end_inset

go together
\begin_inset Quotes erd
\end_inset

 have a high MI.
 For example, he finds that the MI for 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 to be very high; this has to do with the presence of the word–pair 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 in many newspaper stories of the era.
 
\end_layout

\begin_layout Standard
This allows him to propose a parsing algorithm that gives results rather
 close to those proposed by linguists, in which a word such as 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

noun
\begin_inset Quotes erd
\end_inset

, while 
\begin_inset Quotes eld
\end_inset

Northern
\begin_inset Quotes erd
\end_inset

 is taken as an adjective, modifying 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

.
 The resulting parses are of the form of a dependency grammar, 
\emph on
a la
\emph default
 Lucien Tesnière, and more generally along the many rich elaborations of
 dependency grammars over many decades, hundreds of books and far more papers.
 An example parse, taken from Yuret's thesis, is shown below.
 The numbers above the word pairs are the numerical MI value obtained from
 counting.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename agi-2022-tex/Yuret.eps

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The parse is obtained by considering all possible trees joining the words,
 and selecting the tree for which the total MI is the largest possible.
 Basic familiarity with linguistics shows that this parse is exactly correct:
 all of the dependencies are indicated exactly as they should be.
 This is remarkable, because it shows that the toil and trouble that linguists
 have gone through to assemble lexis (dictionaries) by hand, indicating
 subject–verb–noun relations, adjective–noun relations, determiner–noun
 relations, prepositions, etc.
 can be fully automated simply by counting and noting the frequency.
 This is a remarkable achievement.
\end_layout

\begin_layout Standard
There are two things that Yuret does not do, and apparently no one else
 of that era, exploring variations on this theme, seem to explore or discover.
 One is that there is a vector embedding, and that this embedding reproduces
 exactly the famous WORD2VEC result of King–Man+Woman=Queen.
 What's odd is that, chronologically, this result could have been discovered
 earlier, but wasn't.
 The second important step is that more complex structures can be recursively
 discovered.
\end_layout

\begin_layout Standard
First, lets look at the vector embedding.
 It is nearly trivial, once you see it.
 
\end_layout

\begin_layout Standard
a 
\end_layout

\begin_layout Standard
xxx 
\end_layout

\begin_layout Standard
vector
\end_layout

\begin_layout Subsection
sensorimotor 
\end_layout

\begin_layout Standard
xxxmotor
\end_layout

\begin_layout Standard
attention 
\end_layout

\begin_layout Standard
associative memory
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
