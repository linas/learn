#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref page
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Structuralism
\end_layout

\begin_layout Author
Linas Vepstas
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset


\end_layout

\begin_layout Date
20 June 2025
\end_layout

\begin_layout Address
\begin_inset script superscript

\begin_layout Plain Layout
*
\end_layout

\end_inset

BrainyBlaze
\end_layout

\begin_layout Abstract
A summary review of a structuralist, agent approach to machine consciousness.
\end_layout

\begin_layout Section*
Preface / Apologia
\end_layout

\begin_layout Standard
I will attempt to write as simply as possible.
 This means to some of the below might seem simplistic, obvious and well–known.
 If so, then its a mis–perception: attempting to turn the ideas below into
 working software is a half–finished project, with many difficult technical
 issues.
 These are accompanied by even harder questions of system architecture,
 design choice, and, at the most base level, the structural representation
 of what superficially seem like simple, easy obvious ideas.
\end_layout

\begin_layout Standard
Anyone familiar with philosophy or sociology will immediately recognize
 a broad range of familiar concepts: the signifier and signifcand, for example.
 Anyone familiar with mathematics will recognize a different set of familiar
 topics, such as monoidal categories and fragments of linear logic (the
 internal logic of dagger–symmetric tensor categories).
 These resemblances are not accidental; this text is being written in such
 a way as to evoke those ideas, and you the reader should think of them.
 There is much to be said and the author cannot say it all, so it is up
 to the reader to elaborate.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
It appears that the world is made of things and the relationship between
 things.
 These can be represented as graphs, with objects as vertices, and relations
 as edges.
 Graphs are decomposable into parts; a proper decomposition preserves the
 relationships by marking edges, so that the disassembled parts can be reassembl
ed appropriately.
 The marked cut edges imply that the graph parts resemble jigsaw puzzle
 pieces:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename reco-image/sparse-cut.eps
	width 80col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The task of intelligence, of an intelligent agent, is to discover these
 relationships.
 The agent can do so by means of perception; at the lowest levels these
 are concrete: eyes, ears, etc.
 but can also be abstract.
 For example, knowing some facts about geography and politics may still
 leave one ignorant of the relationship between the two, and may require
 a significant intellectual effort to understand this relationship.
 This is again an act of perception: the pattern of relationships between
 politics and geography has to be perceived, although the organ of perception
 is no longer as simple as 
\begin_inset Quotes eld
\end_inset

eyes and ears
\begin_inset Quotes erd
\end_inset

, but is also abstract.
\end_layout

\begin_layout Standard
An intelligent agent perceives and models the external universe, and constructs
 a provisional model of 
\begin_inset Quotes eld
\end_inset

what is outside
\begin_inset Quotes erd
\end_inset

.
 This model conventionally called 
\begin_inset Quotes eld
\end_inset

the world model
\begin_inset Quotes erd
\end_inset

, is necessarily incomplete, hypothetical, and riddled with errors.
 There may be more than 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 model that is active at a time, as one needs to entertain possibilities:
 
\begin_inset Quotes eld
\end_inset

it could be this or it could be that
\begin_inset Quotes erd
\end_inset

.
 This raises a minor philosophical question: is this 
\begin_inset Quotes eld
\end_inset

one
\begin_inset Quotes erd
\end_inset

 world–model, or 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

? After all, hypothetical relationships can be represented in a variety
 of formal logics; the 
\begin_inset Quotes eld
\end_inset

two
\begin_inset Quotes erd
\end_inset

 possibilities can be collapsed down to one grand–total.
\end_layout

\begin_layout Standard
The following is split into several sections.
 
\end_layout

\begin_layout Itemize
The first section reviews the concept of vector representations and their
 relationship to graphical models.
\end_layout

\begin_layout Itemize
The second section reviews
\end_layout

\begin_layout Section
Vector representations
\end_layout

\begin_layout Standard
The multiple possibilities have a natural representation as a vector; such
 a vector representation make natural contact with the conventional frameworks
 of artificial neural nets.
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename struct-image/jigsaw-tensor.eps
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The above is meant to be a diagrammatic representation of a a vctor with
 three basis elements (eyes, lens, telescope) with three (real number) weights
 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

.
 The above is drawn diagrammatically; other common notations used in the
 literature are
\begin_inset Formula 
\begin{align*}
v & =w_{1}\left|\psi_{1}\right\rangle \oplus w_{2}\left|\psi_{2}\right\rangle \oplus w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\left|\psi_{1}\right\rangle +w_{2}\left|\psi_{2}\right\rangle +w_{3}\left|\psi_{3}\right\rangle \\
 & =w_{1}\hat{e}_{1}+w_{2}\hat{e}_{2}+w_{3}\hat{e}_{3}\\
 & =w_{1}e_{1}+w_{2}e_{2}+w_{3}e_{3}\\
 & =\left[w_{1},w_{2},w_{3}\right]\\
 & =X_{1}p\left(X\vert x=X_{1}\right)+X_{2}p\left(X\vert x=X_{2}\right)+X_{3}p\left(X\vert x=X_{3}\right)
\end{align*}

\end_inset

All of these mean exactly the same thing; the notation varies according
 to the style and preference of the authors.
 The last line is meant to be typical of notation used in Bayesian texts,
 where one talks of conditional probabilities; the first line is typical
 of texts on quantum (in which case the weights are complex numbers) and
 sometimes seen in tensor categories and category–theoretic texts.
 The 
\begin_inset Formula $\hat{e}_{k}$
\end_inset

 are simply unit basis vectors, in the conventional linear algebra sense.
\end_layout

\begin_layout Standard
For the present purposes, these differences are immaterial; yes, of course,
 quantum is not the same as Bayesianism, but for now this does not matter.
 The distinctions will be drawn later; for now, the emphasis is that graphical
 elements, such as jigsaws, can be formally combined to form vectors, and
 that distinct graphs are the 
\begin_inset Quotes eld
\end_inset

basis vecttors
\begin_inset Quotes erd
\end_inset

 of the vector space.
\end_layout

\begin_layout Standard
Computationally, the graphical elements depicted in the diagram should be
 stored somewhere, for example, in a graph database; and alongside each
 there should be a floating–point value.
 It can be stored with the graph, or outside the graph, as a distinct vector;
 this is a software engineering issue and not a fundamental issue.
\end_layout

\begin_layout Standard
What do the vector weights 
\begin_inset Quotes eld
\end_inset

mean
\begin_inset Quotes erd
\end_inset

? These are perhaps context–dependent.
 If one is visiting an astronomical observatory, it might make sense to
 set 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.05,0.05,0.9\right)$
\end_inset

 but if one is taking a walk in the park, then perhaps 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)=\left(0.9,0.1,0\right)$
\end_inset

 with 
\begin_inset Formula $w_{2}$
\end_inset

 non–zero because one wears eye–glasses.
 But the 
\begin_inset Quotes eld
\end_inset

actual meaning
\begin_inset Quotes erd
\end_inset

 of this vector is somewhat immaterial; the weights will ultimately be determine
d by whatever algorithm one uses, and whatever 
\begin_inset Quotes eld
\end_inset

meaning
\begin_inset Quotes erd
\end_inset

 that algorithm assigns to weights.
 The algorithm might be some deep–learning neural net transformer; it might
 be some Bayesian system; it might be a frequentist counting algorithm.
\end_layout

\begin_layout Standard
Later on, we will make the conceptual jump that one may work with many different
 algorithms 
\begin_inset Quotes eld
\end_inset

at the same time
\begin_inset Quotes erd
\end_inset

, and that one can (meaningfully) arrange these into a vector.
 The weights 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 might indicate how many CPU cycles should be devoted to algorithms 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 but they might also mean how often these algorithms 
\begin_inset Quotes eld
\end_inset

get the right answer
\begin_inset Quotes erd
\end_inset

.
 The perceptual framing at this more abstract level is 
\begin_inset Quotes eld
\end_inset

how can I perceive which algorithm is the best one?
\begin_inset Quotes erd
\end_inset

 The intelligent agent might be an artificial scientist, running experiments
 to determine which of the scientific hypothesis 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 are correct.
\end_layout

\begin_layout Standard
The implied software design principle here is that the software system must
 be capable of representing 
\begin_inset Quotes eld
\end_inset

objects
\begin_inset Quotes erd
\end_inset

 like 
\begin_inset Formula $e_{1}$
\end_inset

, 
\begin_inset Formula $e_{2}$
\end_inset

 and 
\begin_inset Formula $e_{3}$
\end_inset

 in some abstract way, but also working with vectors and matrices and tensors
 in a natural way: this is the 
\begin_inset Quotes eld
\end_inset

neuro–symbolic
\begin_inset Quotes erd
\end_inset

 idea: the 
\begin_inset Formula $e_{1},e_{2},e_{3}$
\end_inset

 are symbolic, the 
\begin_inset Formula $w_{1},w_{2},w_{3}$
\end_inset

 are neuro.
\end_layout

\begin_layout Standard
The implied context of this text is that the appropriate software system
 for this work is the AtomSpace.
 This claim is founded on the author having spent more than a decade to
 realize the above (and more, some of which touched on below) in a practical,
 efficient, usable and useful way.
 The design is imperfect; new insights are gained regularly.
 Of course, other software frameworks are possible; the engineering task
 is to avoid common pitfalls, design oversights and, of course, re–inventing
 the wheel.
 The AtomSpace is not the be–all, end–all, and it might not be at all suitable
 for whatever you have in mind.
 In such a case, use what you need to use.
 For myself, speaking personally, it is the best that I've got, at the moment.
 The actual stress of daily use does mean that the process is evolutionary,
 and that revisions and enhancements are ongoing.
 Perhaps at a lower rate than before, perhaps limited to narrower sub–modules
 and subsystems.
 The theory drives the practice drives the code and the driving also happens
 in the reverse.
\end_layout

\begin_layout Subsection
Frequentist vector representations
\end_layout

\begin_layout Standard
In the narrow context of deep–learning neural nets, it is widely understood
 the vector embeddings work extremely well, taking the form of modern LLM's
 and a huge variety of applications of transformers to a broad range of
 topics.
 What is not well known is that there are other ways of getting at least
 some of these results.
\end_layout

\begin_layout Standard
The goal of this section is to show how to obtain these results, and to
 provide a very short review of linguistics.
 Now, of course, present–day LLM's far exceed the abilities of present–day
 traditional linguistics; and the latter have fallen into disfavor threatening
 obscurity.
 The ideas developed immediately below is that there is another path, this
 path is explicitly symbolic, and that this path, to the extent that it
 has been pursued, actually reproduces at least some neural–net results.
 
\end_layout

\begin_layout Standard
The original prototype system was WORD2VEC and the prototypical example
 is the vector relation King–Man+Woman=Queen.
 Here, the word–tokens for the words 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Queen
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Woman
\begin_inset Quotes erd
\end_inset

 were given vector embeddings, and the system, a multi–layer recurrent neural
 net (RNN) was trained on a large body of text, using gradient–descent methods.
 After training, the resulting vectors have several interesting properties:
 they exhibit 
\begin_inset Quotes eld
\end_inset

associative memory
\begin_inset Quotes erd
\end_inset

, in that nearby vectors appear to be related, and, much more surprisingly,
 vector subtraction, i.e.
 subtracting the vector for 
\begin_inset Quotes eld
\end_inset

Man
\begin_inset Quotes erd
\end_inset

 from the vector for 
\begin_inset Quotes eld
\end_inset

King
\begin_inset Quotes erd
\end_inset

 seems to give a vector representation for 
\begin_inset Quotes eld
\end_inset

ruler
\begin_inset Quotes erd
\end_inset

 that is gender–neutral, in that 
\begin_inset Quotes eld
\end_inset

Queen–Woman
\begin_inset Quotes erd
\end_inset

 is another vector that is close to 
\begin_inset Quotes eld
\end_inset

King–Man
\begin_inset Quotes erd
\end_inset

.
 Here, 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 means that the cosine product (the vector product) is approximately 1.0.
 Again, this is all well–known.
\end_layout

\begin_layout Standard
What is not well–known is that the above results can be obtained in at least
 one very different way: by frequentist counting.
 Below follows a short sketch of how this is done.
 The focus and lesson here is that the algorithm is not only very different
 from that of RNN's, but is also explicitly graphical (symbolic) in nature.
 This provides the leverage needed to make the jump from neuro to symbolic
 (structural) and back.
\end_layout

\begin_layout Standard
The algorithm described here was provided by Deniz Yuret in his PhD thesis
 in the late 1990's.
 A variety of related formulations were explored by a number of authors
 (including Dekang Lin and Pedro Domingos) over dozens of publications (if
 not more).
 None appear to have made the full conceptual leap being described here,
 although they all danced very close.
 I don't entirely understand why; this is perhaps the intermittent nature
 of scientific discovery.
\end_layout

\begin_layout Standard
The Yuret algorithm starts with a text training corpus, and simply 
\emph on
counts
\emph default
 nearby word–pairs.
 Nearby simply means 
\begin_inset Quotes eld
\end_inset

within a small context window
\begin_inset Quotes erd
\end_inset

, traditionally six words wide.
 This count is written as 
\begin_inset Formula $N\left(w_{a},w_{b}\right)$
\end_inset

 for words 
\begin_inset Formula $w_{a},w_{b}$
\end_inset

 (no relation to the weights 
\begin_inset Formula $w$
\end_inset

 in the previous section.) Yuret defines the mutual information (MI) associated
 with a word–pair as 
\begin_inset Formula 
\[
MI\left(w_{a},w_{b}\right)=\log_{2}\frac{N\left(w_{a},w_{b}\right)N\left(*,*\right)}{N\left(w_{a},*\right)N\left(*,w_{b}\right)}
\]

\end_inset

where 
\begin_inset Formula $*$
\end_inset

is 
\begin_inset Quotes eld
\end_inset

wild-card
\begin_inset Quotes erd
\end_inset

, so that 
\begin_inset Formula $N\left(w_{a},*\right)=\sum_{u}N\left(w_{a},u\right)$
\end_inset

 summed over all right–words 
\begin_inset Formula $u$
\end_inset

.
 Yuret observes that words that 
\begin_inset Quotes eld
\end_inset

go together
\begin_inset Quotes erd
\end_inset

 have a high MI.
 For example, he finds that the MI for 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 to be very high; this has to do with the presence of the word–pair 
\begin_inset Quotes eld
\end_inset

Northern Ireland
\begin_inset Quotes erd
\end_inset

 in many newspaper stories of the era.
 
\end_layout

\begin_layout Standard
This allows him to propose a parsing algorithm that gives results rather
 close to those proposed by linguists, in which a word such as 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

noun
\begin_inset Quotes erd
\end_inset

, while 
\begin_inset Quotes eld
\end_inset

Northern
\begin_inset Quotes erd
\end_inset

 is taken as an adjective, modifying 
\begin_inset Quotes eld
\end_inset

Ireland
\begin_inset Quotes erd
\end_inset

.
 The resulting parses are of the form of a dependency grammar, 
\emph on
a la
\emph default
 Lucien Tesnière, and more generally along the many rich elaborations of
 dependency grammars over many decades, hundreds of books and far more papers.
 An example parse, taken from Yuret's thesis, is shown below.
 The numbers above the word pairs are the numerical MI value obtained from
 counting.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename agi-2022-tex/Yuret.eps
	width 70col%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The parse is obtained by considering all possible trees joining the words,
 and selecting the tree for which the total MI is the largest possible.
 Basic familiarity with linguistics shows that this parse is exactly correct:
 all of the dependencies are indicated exactly as they should be.
 This is remarkable, because it shows that the toil and trouble that linguists
 have gone through to assemble lexis (dictionaries) by hand, indicating
 subject–verb–noun relations, adjective–noun relations, determiner–noun
 relations, prepositions, etc.
 can be fully automated simply by counting and noting the frequency.
 This is a remarkable achievement.
 This result is explored by many authors under the rubric of 
\begin_inset Quotes eld
\end_inset

maximal spanning tree parsing
\begin_inset Quotes erd
\end_inset

 (MST parsing) and is tied to more general principles of 
\begin_inset Quotes eld
\end_inset

maximum entropy principles
\begin_inset Quotes erd
\end_inset

 (MaxEnt).
\end_layout

\begin_layout Standard
There are two things that Yuret does not do, and apparently no one else
 of that era, exploring variations on this theme, seem to explore or discover.
 One is that there is a vector embedding, and that this embedding reproduces
 exactly the famous WORD2VEC result of King–Man+Woman=Queen.
 What's odd is that, chronologically, this result could have been discovered
 earlier, but wasn't.
 The second important step is that more complex structures can be recursively
 discovered.
\end_layout

\begin_layout Standard
First, lets look at the vector embedding.
 It is nearly trivial, once you see it.
 The vector to be written is 
\begin_inset Formula 
\[
\left|King\right\rangle =\sum_{a}c_{a}\left|w_{a}\right\rangle 
\]

\end_inset

where the sum extends over all words in the vocabulary.
 Each word is associated with a basis vector, written here as 
\begin_inset Formula $\left|w_{a}\right\rangle $
\end_inset

.
 This is in the sense of the diagram up top, except that the graph is a
 trivial graph: it is just a single vertex, having the word as the vertex
 label.
 The 
\begin_inset Formula $c_{a}$
\end_inset

 are just floating–point numbers.
 To belabor the point, the sum over the vocabulary can be written as 
\begin_inset Formula 
\[
\left|King\right\rangle =c_{dog}\left|dog\right\rangle +c_{tall}\left|tall\right\rangle +c_{other}\left|other\right\rangle +\cdots
\]

\end_inset

and the Ansatz is that the constants 
\begin_inset Formula $c_{a}$
\end_inset

 can be taken to be 
\begin_inset Formula $c_{a}=MI\left(w_{a},King\right)$
\end_inset

.
 This gives a bona–fide vector, and, as a vector one can take dot products,
 and compute the cosine angles between them, and, when one takes care to
 normalize the vectors to unit length, one recovers the prototypical vector
 embedding result King–Man+Woman=Queen.
 This holds entirely without any appeal to RNN's or any of the other mechanics
 of deep learning.
 Nor is there any immediate need or appeals to free energy, Boltzmann distributi
ons, Markov properties, integrated information theory, Tononi's Phi or any
 other advanced conceptual frameworks.
 It can be noted (should be noted) that 
\begin_inset Formula $MI\left(w_{a},w_{b}\right)$
\end_inset

 is a matrix (a 2–tensor) and that is is kind–of 
\begin_inset Quotes eld
\end_inset

Markovian
\begin_inset Quotes erd
\end_inset

 in a certain sense, although just how is requires a much fuller theory.
\end_layout

\begin_layout Standard
The second 
\begin_inset Quotes eld
\end_inset

obvious
\begin_inset Quotes erd
\end_inset

 extension to the maximum entropy principle that no one seems to make is
 that it can be extended to obtain the full dependency lexis that is the
 centerpiece of traditional linguistics.
 The basic idea is best illustrated with a very simple diagram:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename skimage/disjunct.eps
	width 60col%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The top row of the diagram show a parse obtained via Yuret's frequentist
 maximal MI parsing algorithm.
 The lower row merely applies a pair of scissors to cut the graph into jigsaws,
 and labels the cut ends so that they can be reconstructed.
 This disgram should be compared to the figure below, taken from the 1991
 paper describing Link Grammar (LG):
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename reco-image/link-grammar.png
	lyxscale 60
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
This figure explicitly shows the jigsaw connectors in a jigsaw–shape form.
 The point of Link Grammar is that if one abides by the rule that matching
 connectors must be mated according to the obvious matching rules, the generated
 sentences are always grammatical.
 The connectors are labelled with the letters D, S, O, standing for Determiner,
 Subject and Object.
 The intransitive verb 
\begin_inset Quotes eld
\end_inset

ran
\begin_inset Quotes erd
\end_inset

 must take a subject (which is a noun); the transitive verb 
\begin_inset Quotes eld
\end_inset

chased
\begin_inset Quotes erd
\end_inset

 takes a subject and an object (both nouns).
\end_layout

\begin_layout Standard
As a theory of grammar, Link Grammar 
\begin_inset Quotes eld
\end_inset

works
\begin_inset Quotes erd
\end_inset

: it has fully developed and complete dictionaries for English, Russian
 and Thai.
 It has a demo dictionaries for German, Arabic and Persian and proof–of–concept
 dictionaries for Vietnamese, Turkish, Lithuanian and Indonesian.
 It works.
\end_layout

\begin_layout Standard
A few quick remarks about notation are in order.
 The formal Link Grammar parse for the prior example would be
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename skimage/parse.ps
	lyxscale 160
	width 30col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align left
The lexical entries, in conventional LG notation are
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball:  D- & O-;
\end_layout

\begin_layout Plain Layout

	Kevin: S+;
\end_layout

\begin_layout Plain Layout

	the:   D+;
\end_layout

\begin_layout Plain Layout

	threw: S- & O+;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
The capital letters, together with a 
\family typewriter
+
\family default
/
\family typewriter
-
\family default
 sign, are called 
\begin_inset Quotes eld
\end_inset

connectors
\begin_inset Quotes erd
\end_inset

; a pair of connectors form a 
\begin_inset Quotes eld
\end_inset

link
\begin_inset Quotes erd
\end_inset

.
 The 
\family typewriter
+
\family default
/
\family typewriter
-
\family default
 sign indicates a left–right directionality: for example, the 
\family typewriter
S+
\family default
 connector can only connect to the right; it must mate with an 
\family typewriter
S-
\family default
 connector to form an 
\family typewriter
S
\family default
 link.
 A valid parse exists if and only if all available connectors are paired
 up.
 The construction 
\family typewriter
S+ & O-
\family default
 is called a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

; the name has a historical basis that is of no particular concern here.
 During parsing, all connectors in a disjunct must be satisfied.
\end_layout

\begin_layout Standard
Each lexical entry is a word–disjunct pair; they are of the general form
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	word: A- & B+ & C+ & ...;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is the notation used in the original LG papers.
 It turns out that this is also an active topic of research in quantum computing
; Bob Coecke gives a quantum algorithm that can parse LG in linear time.
 The notation used in those papers would write 
\begin_inset Formula 
\[
\left|word\right\rangle =\left\langle A\right|\otimes\left|B\right\rangle \otimes\left|C\right\rangle \otimes\cdots
\]

\end_inset

Please note that although the notation is quite different, the intended
 interpretation is exactly the same.
 This is not an accident: it turns out that algebraically, the system of
 jigsaw connectors forms a semi–commutative monoid; the internal logic of
 this category embeds a fragment of linear logic; this fragment is the same
 as that for history and trace monoids used to describe communicating computing
 networks, and the mutex locks needed to serialize access to data.
 But linear logic is the internal logic of dagger–compact categories, with
 are the categories that describe Hilbert spaces and quantum mechanics more
 generally.
 It is for this reason that one can discover quantum computing algorithms
 that perform linguistic parsing: the logic of jigsaw assembly is not just
 a subset of quantum, but is also the correct language for describing communicat
ing computer processes and the message–passing between them.
 Informally, you can think of a jigsaw piece as a computer program, and
 the jigsaw connectors as the ports through which it can communicate.
\end_layout

\begin_layout Standard
This last paragraph consisted of a waterfall of jargon and buzzwords.
 The intent of this paragraph is to make the statement that the jigsaw paradigm
 is fundamental: it has algebraic properties that are exactly what is needed
 to describe a vast array of phenomena.
 The use of jigsaws for linguistics is noted as early as the 1960's; the
 utility for computing isn't noted till decades later, and the connection
 to quantum is not noted until the idea of string diagrams and the Mike
 Stay / John Baez 
\begin_inset Quotes eld
\end_inset

Rosetta Stone
\begin_inset Quotes erd
\end_inset

 is written.
 Perhaps this sounds somehow deep and magical; actually, its not.
 It is simply the observation that if you take a pair of scissors to a graph,
 and cut up the edges, but preserve the labels, you get jigsaws.
 Since, in a sense.
 
\begin_inset Quotes eld
\end_inset

the universe is just a collection of objects and the relations between them
\begin_inset Quotes erd
\end_inset

, it is not a surprise that 
\begin_inset Quotes eld
\end_inset

everything is describable with jigsaws
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
A short note about algorithms.
 The diagram above yielded a jigsaw
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball: the- & threw-;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
whereas the desired LG representation would be
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\ttfamily}"
inline false
status open

\begin_layout Plain Layout

	ball: D- & O-;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
How can one get from individual words, to connector categories? Easy: clustering.
 On can again perform frequentist counting, obtain MI , compute cosine products,
 and create clusters from the resulting 
\begin_inset Quotes eld
\end_inset

associative memory
\begin_inset Quotes erd
\end_inset

.
 This has been done in the OpenCog Learn project, circa 2016–2020, and it
 works fine.
 The tooling for all this exists for the AtomSpace; however, it is now old,
 and there is good reason to redesign it from the ground up, as the next
 sections will review.
\end_layout

\begin_layout Standard
BTW, once again, one has embedding vectors: one can write (for example)
\begin_inset Formula 
\[
\left|King\right\rangle =c_{1}\left|the\right\rangle \otimes\left|threw\right\rangle +c_{2}\left|ruled\right\rangle \otimes\left|Poland\right\rangle +\cdots
\]

\end_inset

and after computing the constants 
\begin_inset Formula $c_{1},c_{2},\cdots$
\end_inset

 appropriately, one will find, once again, that King–Man+Woman=Queen holds
 as a vector relation.
\end_layout

\begin_layout Subsection
Compositionality and recursion
\end_layout

\begin_layout Standard
The jigsaw paradigm allows the structural ideas to be re–applied at higher
 layers of inference.
 This is most easily illustrated with a pair of diagrams.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename reco-image/puzzle-idiom.eps
	lyxscale 50
	width 30col%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename reco-image/anaphora.eps
	lyxscale 70
	width 60col%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The point being illustrated here is that smaller partial assemblages still
 present themselves as structures that can have relations between one–another,
 with open connectors that can be joined together.
 As before, the relations are explicitly identified: the anaphore 
\begin_inset Quotes eld
\end_inset

she
\begin_inset Quotes erd
\end_inset

 explicitly refers to 
\begin_inset Quotes eld
\end_inset

Susan
\begin_inset Quotes erd
\end_inset

, occurring earlier in the text.
 In principle, the same techniques applied at earlier stages to discover
 relationships (the pair–wise relationships between words, and the construction
 of a grammar) can be used again to discern structural relations at this
 higher, more abstract level.
 In principle, this can continue onwards, indefinitely, working at ever–higher
 abstraction layers.
 The above figures are still relatively concrete: 
\begin_inset Quotes eld
\end_inset

she
\begin_inset Quotes erd
\end_inset

 explicitly refers to someone.
 At the multi–paragraph or page level, the structural relationships may
 be at the abstraction level of a social encounter, and at the multi–page
 level, perhaps some romance or perhaps some adventure or murder mystery.
 It should be possible to recursively discern a web of explicit structural
 relationships between increasingly abstract elements.
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

It should be possible
\begin_inset Quotes erd
\end_inset

, of course, carries a lot of hope.
 Code for working with word–pairs and grammatical structures is fully complete,
 debugged and works great.
 There's a docker container that puts all these software pieces together,
 you turn it on and go.
 Initial steps were taken to work the code base to be able to perform these
 higher levels of recursive abstraction.
 A raft of issues then presented themselves, and so it is appropriate to
 halt for a critique at this point.
\end_layout

\begin_layout Subsection
Present status and future directions
\end_layout

\begin_layout Standard
The theories and ideas presented above are old and well–established.
 The claims made, specifically about the vector representations, the behavior
 of the vector and matrix embeddings as a form of associative memory, and
 the ability to extract grammar, this has all been explored and demonstrated,
 and it works.
 I've personally written up an extensive body of papers detailing both the
 theory and presenting the results, analyzing them and putting them in context.
 There is a mature body of code implemented in the AtomSpace to perform
 this counting, compute the MI, compute vector products, perform the high–dimens
ional clustering to obtain categories.
 All this is attached to a generative system that can generate new sentences
 conforming to this grammar.
 Under it all, as a foundational layer, the AtomSpace can save and restore
 the graphs, weights and vectors to and from disk, and ship them around
 over the network, expose them on a network server.
 It works, but a critique is in order.
\end_layout

\begin_layout Standard
Some points, from narrow to broad:
\end_layout

\begin_layout Itemize
The core foundation provided by the AtomSpace is solid.
 It stores graphs and vectors just fine.
\end_layout

\begin_layout Itemize
The computation of MI and assorted products (cosine, Hamming, etc.) is implemente
d as a library.
 It is matrix–oriented: vector elements 
\begin_inset Formula $v_{i}$
\end_inset

 and matrix elements 
\begin_inset Formula $M_{ij}$
\end_inset

 are exposed; these use graphs as basis elements (as explained above) and
 the graph representation allows extremely sparse vectors and matrices (after
 all, 
\begin_inset Quotes eld
\end_inset

Kings
\begin_inset Quotes erd
\end_inset

 are mostly not related to fire–hydrants, stars and Oolong tea.
 Relationships are necessarily sparse.)
\end_layout

\begin_layout Itemize
The library is just that: a library.
 It really should be represented as Atomese, that is, as a graph itself.
 That is, instead of saying that 
\begin_inset Formula $dot\left(v,w\right)$
\end_inset

 is some function call to a subroutine called 
\begin_inset Quotes eld
\end_inset

dot
\begin_inset Quotes erd
\end_inset

, passing two vector arguments 
\begin_inset Formula $v$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

, it should instead itself be represented as a graph, as a jigsaw, so that
 
\begin_inset Formula $v$
\end_inset

 and 
\begin_inset Formula $w$
\end_inset

 are input jigsaw connectors, connecting to vectors, and 
\begin_inset Quotes eld
\end_inset

dot
\begin_inset Quotes erd
\end_inset

 is an output connector, connecting to anything that wants a real number
 as input.
\end_layout

\begin_layout Itemize
The goal of representing the algorithmic elements (dot products, Hamming
 distances, etc.) as jigsaws is so that meta–algorithms can themselves connect,
 change and re-arrange what is connected to what.
 That is, although MI, the mutual information, is a good way to measure
 affinity, there are in fact a large number of similar measures, some of
 which might be more accurate, and others which might be faster.
 The idea is that these should not be explored by human software engineers
 vibe–coding with Claude, but by meta–algorithms, perhaps genetic algorithms,
 assembling these (jigsaw) pieces.
\end_layout

\begin_layout Itemize
The above idea of implementing algorithms as processing pipelines is partly
 implemented; mostly implemented at the base layer.
 The generative parts, where pipeline elements can be reconnected by meta–algori
thms, this has not been implemented.
 Both MOSES and AS-MOSES serve as a proof of concept of this idea, but need
 a fundamental redesign to be generally applicable.
 A later part of this paper will expand on this, but there is a mire of
 engineering and design issues to be encountered here.
\end_layout

\begin_layout Standard
All of the above–described results are obtained by counting.
 None of the current counting algos run on GPU's, nor is it obvious how
 to port them to GPU's.
 This is an open, unresolved question.
 The primary strength of the LLM & transformer algos is that they are extremely
 parallelizable and can be implemented on GPU's very efficiently.
 It is not at all clear how to do this for counting.
\end_layout

\begin_layout Standard
The tease is even worse, in some ways: although vectors and vector embeddings
 appear, and there is more than just superficial resemblance to RNN's, the
 precise details of the correspondence remain shrouded in mist.
 Perhaps a strong correspondence can be made between the above tensor framework,
 and RNN's, and perhaps not.
 I have personally attempted to find such a correspondence, but haphazardly,
 without much of an effort, as my attention is focused elsewhere.
 Still, having such a correspondence would be a breakthrough, as it would
 build a bridge between LLM's that currently reign supreme, and more traditional
 symbolic computing.
\end_layout

\begin_layout Subsection
Perception, action and agency
\end_layout

\begin_layout Standard
This will be the topic of the next major section, but the motivation for
 it appears at this point, where one attempts to jump from the lower to
 the higher abstraction layers.
\end_layout

\begin_layout Standard
Conventional training involves the force–feeding of a blob of text through
 an algorithmic system.
 For he Yuret pair–counting pipeline, it is just that: counting word pairs.
 Upon completion, the MI can be computed.
 After the MI has been computed, a second pass of the textual data can be
 made, this time performing Maximum Spanning Tree (MST) parsing (or Maximum
 Planar Graph (MPG) parsing).
 This time, the disjuncts are counted, and upon completion, the MI can be
 computed, and the resulting vectors placed into the lexis (specifically,
 the Link Grammar (LG) lexis) where it can be used to generate text (random
 grammatical sentences).
 Anaphora resolution would require a third pass.
 Each pass gets progressively longer, uses more CPU time and is a bit more
 fragile.
 The more abstract things get, the vaguer and messier the results, the less
 certain they seem to be, the less accurate.
 Perhaps a larger corpus solves all? Perhaps a longer training time? These
 issues sound exactly like the kinds of issues faced by LLM's, transformers,
 and almost all other machine learning systems.
 The simple answer is to just throw more data at it, more CPU, crunch longer
 and harder.
 But there is a better way.
\end_layout

\begin_layout Standard
One desire is to use feedback from higher layers to correct or amplify results
 from lower layers.
 The pipeline approach does not allow this.
 Another desire is to continue reading more text, feeding and expanding
 the lower layers, even as processing continues at the higher layers.
 Again, pipelineing does not allow this.
 Incremental ingestion of data also means that things like MI must also
 be updated incrementally; the pipeline computes it in one giant batch job
 (that takes hours to run, depending on the dataset size).Incremental ingestion
 also suggests that partial results should be saved to disk every so often.
 For several reasons: The system may crash or lose electric power; one wants
 to save at least some results.
 A bug might render later data into garbage; one would like to be able to
 rewind to earlier forms, without having to restart from scratch.
 Finally, the system seems to work best if the training done at each stage
 is on the same dataset as the earlier stages: the grammar induction should
 be done on the same texts as for which word–pair counting was done.
\end_layout

\begin_layout Standard
All this put together suggests that there should be some sensori–motor system
 put into place.
 The 
\begin_inset Quotes eld
\end_inset

sensory
\begin_inset Quotes erd
\end_inset

 part of this can be as simple as reading all of the files in a directory.
 The 
\begin_inset Quotes eld
\end_inset

motor
\begin_inset Quotes erd
\end_inset

 part of this is as simple as changing directories.
 Finally, there is a very minimalist 
\begin_inset Quotes eld
\end_inset

world model
\begin_inset Quotes erd
\end_inset

: it is very small and simple: it consists of filenames (filepaths, or URL's),
 plus a collection of bit–flags: has pair counting been run on this file,
 yet? Has MI been done yet, for this pair data? What about grammar induction?
 Anaphora or higher level counting? And maybe more: when is the last time
 this file was touched? Perhaps this file is contains similar contents to
 another file? Perhaps even its an exact duplicate (because we're crawling
 over web–site mirrors, or backup copies, or other reasons for duplicated
 files?)
\end_layout

\begin_layout Standard
These three parts put together suggests that there is (should be) an agent:
 this agent perceives files, remembers basic facts about files, and makes
 decisions to change directories, or to revisit files for a second or third
 scan.
\end_layout

\begin_layout Standard
Once one has the idea of a file system agent firmly rooted in mind, it is
 easy to imagine more general agents: perhaps chatbot agents that can hop
 onto chat channels, or twitter/bluesky/facebook agents that can work with
 social media posts, 
\begin_inset Quotes eld
\end_inset

sensing
\begin_inset Quotes erd
\end_inset

 (reading) and 
\begin_inset Quotes eld
\end_inset

moving
\begin_inset Quotes erd
\end_inset

 (going to different channels, or following different users), and remembering
 some basic status info about the interactions so far.
 
\end_layout

\begin_layout Standard
This naturally leads to the next major question: what is the appropriate
 design for an agent, in general? What is a generic description of sensory
 capabilities, and movement capabilities? Hand in hand with these questions
 comes the realization that such agents cannot be hand–coded, and must not
 be engineer–designed.
 This in turn implies that the agents must be coded in Atomese themselves,
 represented as graphs, as dataflow arrangements of jigsaw pieces, with
 the specific agents being generated according to a grammar of what parts
 of the agent pipeline can be connected to what (e.g.
 vector outputs must be connected to vector inputs; things that expect strings
 must not be connected to things that expect floats, and so on: there is
 a connectivity grammar, of what can be attached to what.
\end_layout

\begin_layout Standard
That is, agents should be auto–generated, explored and measured for quality,
 accuracy, speed and effectiveness; mutated, assembled and recombined, according
 to the the alloweed grammatical rules.
 This opens a new vista of design and representation questions.
 This is just a first taste, a later section will go into more detail.
 BTW, portions of such an agent system have already been prototyped in Atomese.
\end_layout

\begin_layout Standard
But first, lets get back to some baser, more grounded topics, to give a
 better idea of what other kinds of processing might be encountered.
\end_layout

\begin_layout Subsection
Vision
\end_layout

\begin_layout Standard
The above uses natural language and grammar as the base example; but the
 concept generalizes.
 This is most easily communicated in a few pictures.
\end_layout

\begin_layout Frame
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename reco-image/traffic-lights.jpg
	width 20col%

\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Graphics
	filename reco-image/traffic-light-grammar.eps
	width 70col%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
Shapes have a structural grammar.
 The connectors can specify location, color, shape, texture.
 It's not about pixels, its about element groupings into recognizable, recurring
 arrangements.
 
\end_layout

\begin_layout Standard
It is also explicitly symbolic.
 Each jigsaw piece identifies a specific element of the picture.
 It is identified in a form that we humans are already comfortable in working
 with: the round thing in middle, that's the yellow light.
 As a jigsaw, and its relationship with it's surroundings, it's accessible
 to reasoning, to logic.
 This is very different from the situation with neural nets, where you have
 collections of weights to deal with, and not the objects, the symbolic
 representations of the objects themselves.
\end_layout

\begin_layout Subsection
Sound
\end_layout

\begin_layout Standard
The diagram on the left is a sonogram recording of a whale song; on the
 right is a jigsaw representation of an audio filter sequence that is capable
 of recognizing the form and structure of that whale–song.
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Frame
\align center
\begin_inset Graphics
	filename reco-image/noaa-fisheries-humpback.jpg
	width 65col%

\end_inset


\begin_inset Graphics
	filename reco-image/audio-graph.eps
	width 30col%

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
The goal of this illustration is to show that audio elements can also be
 decomposed into elements of a symbol nature.
 Roughly speaking, the diagram on the right can be thought of as a representatio
n of an electronic circuit, an electronic wiring diagram, a way of hooking
 together digital signal processing (DSP) functions capable of triggering
 on the whale song.
 The claim is that the jigsaw paradigm has legs: its a generic concept applicabl
e in broad domains.
\end_layout

\begin_layout Subsection
Critique redux
\end_layout

\begin_layout Standard
The vision and sound processing diagrams above present several new challenges.
 Starting at the narrowest:
\end_layout

\begin_layout Itemize
The library that implements the MI, dot–product, Hamming–distance, etc.
 functions is only partly pipelined up.
 It is not GPU–ready.
 Optimization, data flow, performance is a concern.
\end_layout

\begin_layout Section
sensorimotor 
\end_layout

\begin_layout Standard
xxxmotor
\end_layout

\begin_layout Standard
attention 
\end_layout

\begin_layout Standard
a more important topic: the idea of sensorimotor agents and the recursive
 arrangements of these near the critical point.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
