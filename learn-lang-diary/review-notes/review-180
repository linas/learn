
Review for Submission 180

Overall:
--------
Weak accept. The premise of unifying quantum and classical descriptions
of computation is quite interesting. However, the paper under-delivers:
it attempts to pack far too much material into too small a space. Early
parts of the paper make unpalatable glosses that appear to contain
errors and are a bit vague. The middle part is a recapitulation of
standard classroom material. The final part, which should have been
where the "good stuff" could be found, fails to make contact with the
conventional formulations of logic and reasoning (for the classical
case) and with the conventional formulation of quantum computing
(for the quantum case). As a whole, the paper is poetic and suggestive,
but the lack of specificity puts the onus on the reader to actually
develop the theory in a meaningful way.


Troublesome statements:
-----------------------
Page 2: "Classical states are then ρ which are diagonal in the basis of X ."

    This is not correct. Consider the pure state |a>. The corresponding
    density operator is ρ = |a><a| which is clearly diagonal, but one
    would never call a pure state "classical"; it is just a pure state.

    I think the intent was to say something along the lines of
    "The observables of a classical system form a commutative
    C*-algebra" or perhaps "The observables of a classical system
    commute with density operators that are diagonal in the basis of X"

    This has knock-on effects into the definitions of CTC, CTQ etc.
    A channel is usually understood to be a map between C*-algebras;
    a classical channel is a map of a commutative C*-algebra.

    So I think the intended meaning here is something like this:
    "The space L(H) of linear operators on the Hilbert space H can
    be decomposed into four parts. Select a subspace of C ⊆ L(H)
    that is commutative; this is the "classical" subspace; the
    (orthogonal?) complement is the "quantum subspace" Q. The space
    channels Psi:L(H) -> L(H) then splits into CTC, CTQ, QTC, QTQ
    which are the maps between these subspaces."

    I think that is the intent. Now, in general, there won't be
    commutative subspaces, unless you explicitly make it so; so
    for example, I think you want the space H = C^|Σ|⊗ R^|Σ| where
    R^|Σ| is real Euclidean space, used to model the space of classical
    registers, C^|Σ| is used for the quantum states, and now the
    collection of automorphisms of the C*-algebra L(H) to itself
    splits into the desired CTC, CTQ, QTC, QTQ parts, which are just
    CTC:L(R^|Σ|) -> L(R^|Σ|)  and CTQ:L(R^|Σ|) -> L(C^|Σ|)
    and QTC:L(C^|Σ|) -> L(R^|Σ|) and QTQ:L(C^|Σ|) -> L(C^|Σ|)

    This is my best guess as to the intent of these definitions but
    perhaps I misunderstood or garbled something here.

    It should be pointed out here that the state transition functions
    of an (ordinary) finite state machine are not linear. However, by
    using the formalism of a probabilistic automaton, you can make it
    linear (and every finite classical Turing machine has a finite
    state machine equivalent.) Then I think what you are shooting for
    in the quantum half is the "quantum finite automaton". See
    https://en.wikipedia.org/wiki/Quantum_finite_automaton and
    https://en.wikipedia.org/wiki/Probabilistic_automaton
    I think these two articles describe the intended split of the
    computation into classical and quantum "registers".

    I have never actually contemplated combining these two descriptions
    into a single unified C*-algebra, and it would be perhaps a good
    exercise to confirm that the detailed formalisms all pass through
    exactly as intended (i.e. as direct products, semi-direct products,
    tensor products, etc. in all those places where they would need to
    appear.)

    Sections 3 and 4 on pages 3-5 are recapitulations of standard
    textbook material; it would have been nice if instead this space
    was devoted to articulating the details of the unified description
    of quantum and probabilistic automata.

    Sections 3 and 4 are also mis-labelled: they have nothing to do
    with AGI, or even with computing: they summarize standard school
    curriculum for physics, with the three letters AGI sprinkled in.
    The space could have better been devoted to making contact with
    e.g. the theory of automata, and then asserting "AGI will be on
    a substrate of these kinds of automata"

Section 5ff.
    This section attempts to pack too much material in too small a
    space. The use of jargon does not help. For example, page 5, bottom:

    "In CAGI it equals a quadratic loss surface over an information
     manifold."

    I think this is trying to say what the next sentence says: the
    classical mutual information is given by the Fisher metric. And
    yes, metrics are quadratic. The location of the "surface" is
    unclear. And the word "loss" has a specific technical meaning in
    deep learning neural nets, where it refers to the Gibbs measure.
    Sure, I suppose one can relate the Gibbs measure "loss" to, I guess,
    some Gaussian on some abstract Wiener space or something like that,
    but unpacking all this could be a short paper in itself. Condensing
    it like this will leave almost all readers confused.

Top of page 6:
    "Classical form (HC ind)."
    "Given data D = {(si, ri)}N i=1, model fθ , weights λ"

    This appears to be some sort of Hamiltonian expression that might
    possibly be used for parametric predictive modelling, e.g. to train
    some deep learning neural net.

    This seems to raise a category error.
    * Category 1: Hamiltonians that are used to describe the time
      evolution of systems. This includes Hamiltonians that describe
      a ball rolling down an inclined plane, Hamiltonians that describe
      mechanical calculators, Hamiltonians that describe transistors and
      computers, Hamiltonians that describe ergodic and mixing processes,
      such as the rings of Saturn or the mixing of rum and coke to form
      a Cuba Libre. The last example is one of a dissipative process.

    * Category 2: Machine learning systems that minimize an objective
      function. It is often the case the objective function is smooth,
      and arises from some integrability condition. In this case, one
      can call the resulting foliation a "Hamiltonian", but this is
      besides the point: in machine learning, one strives to minimize
      the objective function. When it has a foliation, then one can use
      gradient descent. The gradient-descent is *orthogonal* to the
      surfaces of constant energy, the algos search for downhill paths.

   The category error is the confusion between these two. In the first
   category, energy is conserved; in the second case, the word "energy"
   is a linguistic flourish, and most certainly, the "energy" is NOT
   conserved, and the algo specifically attempts to minimize the energy.

   A (classical) AGI agent would necessarily belong to category 1; it
   might use an algorithm from category 2, if the goal of that agent was
   to create a world-model using parametric predictive modelling. But
   that is a big "if". Sure, the current deep-learning neural net
   industry is centered on parametric predictive modelling, but there
   are certainly other ways of doing AI.

   However, this paper claims to want to unify classical and quantum
   models of computing. Confusing the classical computing, which happens
   in category 1, with specific trade-industry algorithms belonging to
   category 2 ... is just ... confusing.

   Some narrower remarks:
   -- What's s_i? What's r_i? The model fθ appears to be a
      "parametric predictor with weights θ and sample-weights λi;"
      Is this formula supposed to be immediately recognizable to
      readers who do parametric predictive modelling? If I squint,
      then it looks like something I've seen before, but I don't
      recognize it as some generic form of the utility function that
      everyone uses.

   -- Is there some intent to say that "every AGI agent will always
      create a world model, and that every world model is necessarily
      equivalent to a parametric predictive model"? If so, both parts
      of such a claim are remarkable and dramatic, and require strong
      support. It is often presumed, but hardly obvious that AGI
      agents will create world-models. Some will, but not necessarily in
      general. To then claim that such a model is equivalent to some
      other parametric predictive model, this too is ... possible, but
      hardly obvious. Both parts of such a claim require a deep leap of
      faith.

Section 5.2:
      "Boolean clauses φα : T∗C → {0, 1}"

      How should one make contact between this definition of a "boolean
      clause" (which seems to be nothing more than an indicator function
      on the cotangent bundle) with more conventional concepts like
      Aristotelean logic or predicate logic or lambda calculus or pi
      calculus, or even a mechanical calculator or a Babbage analytical
      engine? Clearly, a stream-driven analytical engine would be
      classical, but how it is that one should map it's operation to
      a collection of indicator functions on the cotangent bundle is
      unfathomable to me. I suppose it's possible, but I've never seen
      such a treatment, nor is a reference given.

Section 5.2:
      "In the quantum case, clauses lift to projectors Πα on HA:"

      In the conventional articulation of computation (and thus
      reasoning) in the quantum case, as described by Crutchfield
      (in the 1990's) and by many others, one works with a collection
      of unitary operators U_n = exp(i H_n) for a collection of
      non-commuting Hermitian operators H_n, and then considers
      computational sequence U_a U_b U_c ... U_k |ψ> where each
      of the U_j are selected from a state transition system given
      by some geometric finite automaton (the letters j being
      selected from the "alphabet" Σ defined on page 2. Only in
      the final measurement, when the quantum system stops,
      does one apply a projection operator Π to the final state
      U_a U_b U_c ... U_k |ψ> so that the result of the computation
      is Tr (Π U_a U_b U_c ... U_k ρ) for the density matrix ρ.

      I don't understand how to make contact between this conventional
      form, and equation 10.

Rest of paper
   I don't know what to make of the rest of the paper. The first
   half presents a number of formal difficulties that makes it hard
   to plow on and try to figure out what the rest of it means. It
   would have been better if the paper took a more leisurely pace,
   and was careful to define terms and symbols and operations,
   skipping textbook-obvious sections like sections 3 and 4, while
   being much more thoughtful and careful in section 5 to make contact
   with conventional formulations of these topics.


Typos:
------
Page 1 first paragraph: spurious whitespace in "architect ed"
Page 1 missing references: "ontology [?, ?, ?, 4, 8, 20]"
       "of these operators [?]."
       "contextuality [?,3],"

Page 2 "agents and agent are described"
       "We define a Hilbert space X = C|Σ|" -- I assume C is meant to
       stand for the complex numbers. Please use \mathbf{C} to avoid
       ambiguity.
